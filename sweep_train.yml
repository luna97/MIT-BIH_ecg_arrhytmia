program: sweep.py
method: random
name: train-xLSTM
metric:
  goal: maximize
  name: test_f1
parameters:
  pretrain:
    value: False
  lr_head:
    value: 0.001
  lr_xlstm:
    value: 0.00001
  wd:
    value: 0.1
  batch_size:
    value: 128
  num_workers:
    value: 32
  epochs:
    value: 15
  dropout:
    min: 0.1
    max: 0.6
  use_tab_data:
    value: False
  optimizer:
    value: "adamw"
  patch_size:
    value: 64
  embedding_size:
    value: 784
  activation_fn:
    value: 'relu'
  wandb_log:
    value: True
  random_shift:
    values: [True, False]
  data_folder_mit:
    value: '/media/Volume/data/MIT-BHI/data/'
  use_scheduler:
    value: True
  num_epochs_warmup:
    value: 1
  num_epochs_warm_restart:
    value: 15
  deterministic:
    value: True
  is_sweep:
    value: True
  grad_clip:
    value: 5
  patience:
    value: 5
  nk_clean:
    value: True
  normalize:
    value: True
  xlstm_config:
    value: ['m', 's', 'm', 'm', 'm', 'm', 'm', 's', 'm', 'm', 'm', 'm']
  sched_decay_factor:
    value: 0.8
  weight_tying:
    value: True
  use_class_weights:
    values: [True, False]
  split_by_patient:
    values: [True, False]
  num_heads:
    value: 4
  checkpoint:
    value: 'pretrain-xLSTM/jg7je6hh/checkpoints/epoch=38-step=174330.ckpt'
  oversample:
    value: False
  random_drop_leads:
    values: [0., 0.1, 0.2, 0.5]
  leads:
    value: ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
  contrastive_loss_lambda:
    values: [0., 0.1, 1.0, 3.0, 10.0]
  random_jitter_prob:
    values: [0., 0.1, 0.2, 0.5]
  random_surrogate_prob:
    values: [0., 0.1, 0.2, 0.5]
  loss_type:
    value: ''